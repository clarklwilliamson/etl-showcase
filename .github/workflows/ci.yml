name: CI Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  PYTHON_VERSION: "3.11"

jobs:
  lint:
    name: Lint & Format Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install linting tools
        run: |
          pip install ruff black

      - name: Run Ruff (linting)
        run: ruff check dags/ spark_jobs/ tests/

      - name: Run Black (format check)
        run: black --check dags/ spark_jobs/ tests/

  validate-dags:
    name: Validate Airflow DAGs
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install "apache-airflow==2.8.1" \
            "flask-session>=0.5.0,<1.0" \
            apache-airflow-providers-postgres \
            apache-airflow-providers-apache-spark \
            pyspark pandas requests

      - name: Validate DAG syntax and imports
        run: |
          python -c "
          import sys
          import warnings
          from pathlib import Path

          # Suppress deprecation warnings for CI
          warnings.filterwarnings('ignore', category=DeprecationWarning)

          # Add dags to path
          sys.path.insert(0, 'dags')

          errors = []
          for dag_file in Path('dags').glob('*.py'):
              try:
                  exec(open(dag_file).read())
                  print(f'✓ {dag_file.name} - valid')
              except Exception as e:
                  errors.append(f'✗ {dag_file.name} - {e}')
                  print(f'✗ {dag_file.name} - {e}')

          if errors:
              sys.exit(1)
          print('✓ All DAGs validated successfully')
          "

  test-spark:
    name: Test Spark Jobs
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: Install dependencies
        run: |
          pip install pyspark==3.5.0 pytest pytest-cov

      - name: Run Spark job tests
        run: |
          pytest tests/ -v --cov=spark_jobs --cov-report=term-missing

  test-sql:
    name: Validate SQL Schemas
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Validate SQL syntax
        env:
          PGPASSWORD: test
        run: |
          for sql_file in sql/*.sql; do
            echo "Validating $sql_file..."
            psql -h localhost -U test -d test -f "$sql_file"
          done
          echo "✓ All SQL files valid"

  docker-build:
    name: Validate Docker Build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Validate docker-compose.yml
        run: docker compose config --quiet

      - name: Build Airflow image
        run: docker build -f Dockerfile.airflow -t etl-showcase-airflow .
