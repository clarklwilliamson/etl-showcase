name: CI Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  PYTHON_VERSION: "3.11"

jobs:
  lint:
    name: Lint & Format Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install linting tools
        run: |
          pip install ruff black

      - name: Run Ruff (linting)
        run: ruff check dags/ spark_jobs/ tests/

      - name: Run Black (format check)
        run: black --check dags/ spark_jobs/ tests/

  validate-dags:
    name: Validate Airflow DAGs
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install "apache-airflow==2.8.1" \
            "flask-session>=0.5.0,<1.0" \
            apache-airflow-providers-postgres \
            apache-airflow-providers-apache-spark \
            pyspark pandas requests

      - name: Validate DAG syntax and imports
        env:
          AIRFLOW_HOME: ${{ github.workspace }}
          AIRFLOW__CORE__LOAD_EXAMPLES: "false"
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////tmp/airflow.db
        run: |
          # Initialize DB
          airflow db migrate

          # List DAGs (validates parsing)
          airflow dags list

          # Check for import errors
          airflow dags list-import-errors --output json | python -c "
          import sys, json
          data = json.load(sys.stdin)
          if data:
              print('DAG import errors found:')
              for err in data:
                  print(f\"  {err['file']}: {err['error']}\")
              sys.exit(1)
          print('✓ No DAG import errors')
          "

  test-spark:
    name: Test Spark Jobs
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: Install dependencies
        run: |
          pip install pyspark==3.5.0 pytest pytest-cov

      - name: Run Spark job tests
        run: |
          pytest tests/ -v --cov=spark_jobs --cov-report=term-missing

  test-sql:
    name: Validate SQL Schemas
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Validate SQL syntax
        env:
          PGPASSWORD: test
        run: |
          for sql_file in sql/*.sql; do
            echo "Validating $sql_file..."
            psql -h localhost -U test -d test -f "$sql_file"
          done
          echo "✓ All SQL files valid"

  docker-build:
    name: Validate Docker Build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Validate docker-compose.yml
        run: docker compose config --quiet

      - name: Build Airflow image
        run: docker build -f Dockerfile.airflow -t etl-showcase-airflow .
